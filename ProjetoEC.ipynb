{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Modelo de Classificação\n",
    "\n",
    "### Projeto realizado por :\n",
    "\n",
    "#### \n",
    "* Cosmin Trandafir - 57101\n",
    "* Martim Baptista - 56323\n",
    "* João Serafim - 56376\n",
    "* Martim Paraíba - 56273\n",
    "***\n",
    "\n",
    "#### Counter de horas I guess :{}\n",
    "\n",
    "* Cosmin Trandafir - 4h\n",
    "* Martim Baptista - \n",
    "* João Serafim - 3h\n",
    "* Martim Paraíba - \n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questoes: \n",
    "- Devemos apagar colunas que possuem demasiados valores a zero?\n",
    "- Devemos normalizar primeiro ou inputar primeiro?\n",
    "- Devemos testar e mostrar os testes de todos os hyperparametros ou apenas os principais?\n",
    "\n",
    "- Devemos testar com IterativeImputer?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neste projeto vamos usar o dataset: ***biodegradable_a.cvs*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load biodegradable dataset\n",
    "\n",
    "bio_df = pd.read_csv(\"biodegradable_a.csv\")\n",
    "bio_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic functions and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, PowerTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_scores(y_test, y_pred):\n",
    "    # Evaluate the performance of the model using various metrics\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(y_test, y_pred))\n",
    "    print(\"The Recall is: %7.4f\" % recall_score(y_test, y_pred))\n",
    "    print(\"The F1 score is: %7.4f\" % f1_score(y_test, y_pred))\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews_corrcoef(y_test, y_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(imputer_tuple, scaler_tuple, classifier, X_train, X_test, y_train):\n",
    "    imputer = imputer_tuple[1]\n",
    "    # Impute missing values\n",
    "    imputer.fit(X_train)\n",
    "    X_train_imputed = imputer.transform(X_train)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "    \n",
    "    scaler = scaler_tuple[1]\n",
    "    # Scale the test data\n",
    "    scaler.fit(X_train_imputed)\n",
    "    X_train_scaled = scaler.transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_scaled, y_train)\n",
    "    preds = classifier.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    precision = precision_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    \n",
    "    # Compute cross-validation scores\n",
    "    cv_scores = cross_val_score(classifier, X_test_scaled, y_test, cv=10)\n",
    "    mean_cv_score = cv_scores.mean()\n",
    "    \n",
    "    print(\"Imputer: {} \\nScaler: {}\\nCrossValidationScore: {:.6f}\\n\".format(imputer_tuple[0], scaler_tuple[0], mean_cv_score))\n",
    "    return mean_cv_score\n",
    "\n",
    "\n",
    "    \"\"\"  # Create histogram\n",
    "    labels = ['Precision', 'Recall', 'F1 Score', 'MCC', 'Mean CV Score']\n",
    "    scores = [precision, recall, f1, mcc, mean_cv_score]\n",
    "    plt.bar(labels, scores)\n",
    "    plt.title('Model Evaluation Metrics')\n",
    "    plt.ylim([0.7, 1.0])\n",
    "    plt.grid(axis='y')\n",
    "    plt.show() \"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide Freatures and Class columns for preprocessing\n",
    "X = bio_df.iloc[:, :-1]\n",
    "y = bio_df.iloc[:, -1]\n",
    "\n",
    "# Encode string classes to a numeric value for Imputer\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_encoded_classes = le.fit_transform(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df_encoded_classes, test_size=0.33)\n",
    "\n",
    "imputers =[(\"SimpleImputer = mean\", SimpleImputer(strategy='mean')), \n",
    "           (\"SimpleImputer = median\", SimpleImputer(strategy='median')), \n",
    "           (\"KNNImputer 3\",KNNImputer(n_neighbors=3)), \n",
    "           (\"KNNImputer 5\",KNNImputer(n_neighbors=5)), \n",
    "           (\"KNNImputer 9\",KNNImputer(n_neighbors=9)),\n",
    "           (\"KNNImputer 11\",KNNImputer(n_neighbors=11)),\n",
    "           (\"IterativeImputer\",IterativeImputer())]\n",
    "\n",
    "scalers = [(\"MinMaxScaler\",MinMaxScaler()), \n",
    "           (\"StandarScaler\",StandardScaler()), \n",
    "           (\"Normalizer\",Normalizer()), \n",
    "           (\"PowerTransformer\",PowerTransformer())]\n",
    "\n",
    "combos = [imputer_name + \" + \" + scaler_name for imputer_name, imputer in imputers for scaler_name, scaler in scalers]\n",
    "\n",
    "scores = []\n",
    "for imputer in imputers:\n",
    "    for scaler in scalers:\n",
    "        scores.append(evaluate_model(imputer, scaler, LogisticRegression(max_iter=10000), X_train, X_test, y_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 13))\n",
    "ax.bar(combos, scores)\n",
    "ax.set_title('Model Evaluation Metrics')\n",
    "ax.set_ylim([0.8, 1.0])\n",
    "ax.grid(axis='y')\n",
    "ax.set_xticklabels(combos, rotation=90, ha='center')\n",
    "\n",
    "max_score_idx = np.argmax(scores)\n",
    "highest_name, highest_score = combos[max_score_idx], scores[max_score_idx]\n",
    "ax.axhline(y=scores[max_score_idx], color='r', linestyle='--', label='Max score')\n",
    "\n",
    "ax.legend([f'Max score: {highest_name}: {highest_score:.4f}'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [(\"MinMaxScaler\",MinMaxScaler()), \n",
    "           (\"StandarScaler\",StandardScaler()), \n",
    "           (\"Normalizer\",Normalizer()), \n",
    "           (\"PowerTransformer\",PowerTransformer())]\n",
    "\n",
    "for scaler in scalers:\n",
    "    evaluate_model((\"KNNImputer 5\",KNNImputer(n_neighbors=5)), scaler, LogisticRegression(max_iter=10000), X_train, X_test, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, PowerTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Using a simple imputer TODO use the best\n",
    "simple_imputer = SimpleImputer(strategy = \"median\")\n",
    "df_imputed_simple = simple_imputer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_imputed_simple, y, test_size=0.33)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "scalers = [(MinMaxScaler(),\"MinMaxScaler\"), (StandardScaler(),\"StandardScaler\"), (Normalizer(),\"Normalizer\"), (PowerTransformer(),\"PowerTransformer\")]\n",
    "\n",
    "for scaler,name in scalers:\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='RB')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='RB')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Results for:\", name)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "def model_testing(X_train, X_test, y_train, y_test, feature_selection=None):\n",
    "    # select top 5 most correlated variables with y, if feature_selection is not None\n",
    "    if feature_selection == 'correlation':\n",
    "        corr_matrix = np.corrcoef(np.hstack((y_train.reshape((len(y_train), 1)), X_train)).T)\n",
    "        corr_with_y = corr_matrix[0, 1:]\n",
    "        correlationli = list(corr_with_y)\n",
    "        sorted_indices = sorted(range(len(correlationli)), key=lambda k: correlationli[k], reverse=True)\n",
    "        top5 = sorted_indices[:5]\n",
    "        print(\"Top 5 by correlation:\", top5)\n",
    "        X_train = X_train[:, top5]\n",
    "        X_test = X_test[:, top5]\n",
    "\n",
    "    # train models\n",
    "    dtr = DecisionTreeRegressor(max_depth=5)\n",
    "    dtr.fit(X_train, y_train)\n",
    "\n",
    "    lmr = LinearRegression()\n",
    "    lmr.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate models\n",
    "    dt_preds = dtr.predict(X_test)\n",
    "    lr_preds = lmr.predict(X_test)\n",
    "\n",
    "    print(\"RVE DTs: %7.4f\" % explained_variance_score(y_test, dt_preds))\n",
    "    print(\"RVE LRs: %7.4f\" % explained_variance_score(y_test, lr_preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_imputed_simple, df_encoded_classes, test_size=0.33)\n",
    "    \n",
    "# use all variables\n",
    "print(\"------Using all variables------\")\n",
    "model_testing(X_train, X_test, y_train, y_test)\n",
    "print(\"------Using the top5------\")\n",
    "# use top 5 most correlated variables\n",
    "model_testing(X_train, X_test, y_train, y_test, feature_selection='correlation')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise - Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "N,M=X_train.shape\n",
    "#LR\n",
    "lmr=LinearRegression()\n",
    "sfs = SequentialFeatureSelector(lmr, n_features_to_select=5)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "#get the relevant columns\n",
    "features=sfs.get_support()\n",
    "Features_selected =np.arange(M)[features]\n",
    "print(\"The features selected are columns: \", Features_selected)\n",
    "\n",
    "nX_train=sfs.transform(X_train)\n",
    "nX_test=sfs.transform(X_test)\n",
    "\n",
    "model_testing(nX_train, nX_test, y_train, y_test)\n",
    "#--------------------------------------------------------------------\n",
    "#DT Forward\n",
    "dtr = DecisionTreeRegressor(max_depth=3)\n",
    "sfs = SequentialFeatureSelector(dtr, n_features_to_select=5, direction=\"forward\")\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "#get the relevant columns\n",
    "features=sfs.get_support()\n",
    "Features_selected =np.arange(M)[features]\n",
    "print(\"Decision tree: Forward\")\n",
    "print(\"The features selected are columns: \", Features_selected)\n",
    "\n",
    "nX_train=sfs.transform(X_train)\n",
    "nX_test=sfs.transform(X_test)\n",
    "\n",
    "model_testing(nX_train, nX_test, y_train, y_test)\n",
    "#-------------------------------------------------------------------- takes a bit longer\n",
    "#DT backward\n",
    "dtr = DecisionTreeRegressor(max_depth=3)\n",
    "sfs = SequentialFeatureSelector(dtr, n_features_to_select=5, direction=\"backward\")\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "#get the relevant columns\n",
    "features=sfs.get_support()\n",
    "Features_selected =np.arange(M)[features]\n",
    "print(\"Decision tree: Backward\")\n",
    "print(\"The features selected are columns: \", Features_selected)\n",
    "\n",
    "nX_train=sfs.transform(X_train)\n",
    "nX_test=sfs.transform(X_test)\n",
    "\n",
    "model_testing(nX_train, nX_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
